{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_master=fetch_20newsgroups(subset='train', shuffle=1)\n",
    "test_master=fetch_20newsgroups(subset='test',shuffle=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def clean_n_tokenize(doc):\n",
    "    tokens=doc.split()\n",
    "    table=str.maketrans('','',punctuation)\n",
    "    tokens=[w.translate(table) for w in tokens]\n",
    "    tokens=[w for w in tokens if w.isalpha()]\n",
    "    tokens=[w for w in tokens if len(w)>2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=set()\n",
    "vocab={}\n",
    "\n",
    "for doc in train_master.data:\n",
    "    tokens=clean_n_tokenize(doc)\n",
    "    for t in tokens:\n",
    "        if t in vocab:\n",
    "            vocab[t]+=1\n",
    "        else:\n",
    "            vocab[t]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in vocab:\n",
    "    if vocab[w]>10:\n",
    "        words.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113318"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random testing.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(\"This is a test line, for the tokenizer operation\")\n",
    "encoded_docs = tokenizer.texts_to_sequences(\"This is a test line, for the tokenizer operation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'line,', 'for', 'the', 'tokenizer', 'operation']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"This is a test line, for the tokenizer operation\".split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing words from the trimmed corpus, i.e words whose overall frequency in the whole vocab is less than 10.\n",
    "#also puts the line in array format, numpy is faster to work with.\n",
    "train_trimmed=[]\n",
    "for doc in train_master.data:\n",
    "    selected_tokens=[]\n",
    "    tokens=doc.split()\n",
    "    for t in tokens:\n",
    "        if t in words:\n",
    "            selected_tokens.append(t)\n",
    "    line=' '.join(selected_tokens)\n",
    "    train_trimmed.append(line)\n",
    "    \n",
    "test_trimmed=[]\n",
    "for doc in test_master.data:\n",
    "    selected_tokens=[]\n",
    "    tokens=doc.split()\n",
    "    for t in tokens:\n",
    "        if t in words:\n",
    "            selected_tokens.append(t)\n",
    "    line=' '.join(selected_tokens)\n",
    "    test_trimmed.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to embed the text.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def use_embedding(path, encoding):\n",
    "    file=open(path,'r',encoding=encoding)\n",
    "    lines=file.readlines()[1:]\n",
    "    file.close()\n",
    "    embedding=dict()\n",
    "    for l in lines:\n",
    "        words=l.split()\n",
    "        embedding[words[0]]=np.asarray(words[1:],dtype='float32')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enconde the words, then pad'em.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(train_trimmed)\n",
    "train_encoded=tokenizer.texts_to_sequences(train_trimmed)\n",
    "test_encoeded=tokenizer.texts_to_sequences(test_trimmed)\n",
    "\n",
    "max_len=max([len(s.split()) for s in train_trimmed])\n",
    "\n",
    "x_train=pad_sequences(train_encoded,padding='post', maxlen=max_len)\n",
    "x_test=pad_sequences(test_encoeded,padding='post', maxlen=max_len)\n",
    "\n",
    "y_train=keras.utils.to_categorical(train_master.target,num_classes=20)\n",
    "y_test=keras.utils.to_categorical(test_master.target,num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1\n",
    "raw_embiddings=use_embedding('./glove.6B.100d.txt', 'utf8')\n",
    "\n",
    "wt_mat=np.zeros((vocab_size,100))\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    if word in raw_embiddings:\n",
    "        wt_mat[i]=raw_embiddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 6966, 100)         1267100   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 6951, 16)          25616     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3475, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 55600)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 55600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1112020   \n",
      "=================================================================\n",
      "Total params: 2,404,736\n",
      "Trainable params: 2,404,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "embedding_layer=keras.layers.Embedding(vocab_size, 100, weights=[wt_mat], input_length=max_len, trainable=1)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,100, input_length=max_len))\n",
    "model.add(Conv1D(filters=16, kernel_size=16, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "354/354 - 1700s - loss: 2.4703 - accuracy: 0.2034 - val_loss: 1.8505 - val_accuracy: 0.4016\n",
      "Epoch 2/3\n",
      "354/354 - 1139s - loss: 1.1537 - accuracy: 0.6358 - val_loss: 1.4070 - val_accuracy: 0.5786\n",
      "Epoch 3/3\n",
      "354/354 - 1165s - loss: 0.5213 - accuracy: 0.8485 - val_loss: 1.5275 - val_accuracy: 0.6174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x120a241d0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3, verbose=2, validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9427\n",
      "Testing Accuracy:  0.6174\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7813329792883696"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one=Pipeline([('tfidf_v',TfidfVectorizer()),('svm',SGDClassifier(loss='hinge',alpha=1e-3))])\n",
    "model_one=model_one.fit(train_trimmed,train_master.target)\n",
    "\n",
    "model_one_predict=model_one.predict(test_master.data)\n",
    "np.mean(model_one_predict==test_master.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_model_one={'tfidf_v__ngram_range':[(1, 1),(1,2)],\n",
    "                  'tfidf_v__use_idf':(True,False),\n",
    "                  'svm__alpha':(1e-2,1e-3)}\n",
    "gs_model_one=GridSearchCV(model_one,param_model_one,n_jobs=-1)\n",
    "\n",
    "gs_model_one=gs_model_one.fit(train_trimmed,train_master.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8645041541453067\n",
      "{'svm__alpha': 0.01, 'tfidf_v__ngram_range': (1, 2), 'tfidf_v__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "print(gs_model_one.best_score_)\n",
    "print(gs_model_one.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'We': 1,\n",
       " 'are': 3,\n",
       " 'trying': 9,\n",
       " 'to': 8,\n",
       " 'learn': 7,\n",
       " 'NLP': 0,\n",
       " 'is': 6,\n",
       " 'fun': 5,\n",
       " 'and': 2,\n",
       " 'easy': 4}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=['We are trying to learn NLP','NLP is fun and easy']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "vectorizer.fit(text)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 0, 1, 1, 1],\n",
       "       [1, 0, 1, 0, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
